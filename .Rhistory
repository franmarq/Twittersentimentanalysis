library(lubridate)
library(forecast)
library(e1071)
install.packages("pgmm")
install.packages("gbm")
library(pgmm)
library(gbm)
library(pgmm)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
RFfit <- train(diagnosis~ .,data=training,method="rf")
RFpred <- predict(RFfit,testing)
confusionMatrix(RFpred,testing$diagnosis)$overall['Accuracy']
GBMfit <- train(diagnosis~ .,data=training,method="gbm",verbose=FALSE)
GBMpred <- predict(GBMfit,testing)
confusionMatrix(GBMpred,testing$diagnosis)$overall['Accuracy']
LDAfit <- train(diagnosis~ .,data=training,method="lda",verbose=FALSE)
LDApred <- predict(LDAfit,testing)
confusionMatrix(LDApred,testing$diagnosis)$overall['Accuracy']
predDF <- data.frame(RFpred,LDApred,GBMpred,diagnosis=testing$diagnosis)
GAMFit <- train(diagnosis ~.,data=predDF,method="gam")
GAMPred <- predict(GAMFit,predDF)
confusionMatrix(GAMPred,testing$diagnosis)$overall['Accuracy']
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(3523)
install.packages("elasticnet")
library(elasticnet)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
LASSOtraining = concrete[ inTrain,]
LASSOtesting = concrete[-inTrain,]
set.seed(233)
LASSOfit <-  enet(x=data.matrix(LASSOtraining[1:8]),y=LASSOtraining$CompressiveStrength,lambda=1)
plot(LASSOfit, use.color=TRUE)
library(lubridate) # For year() function below
dat = read.csv("c:\music\gaData.csv")
dat = read.csv("c:/music/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tsfit <- bats(tstrain)
tsforecast <- forecast(tsfit,h=nrow(testing))
results <- as.data.frame(tsforecast$lower[,2])
results[,2] <- as.data.frame(tsforecast$upper[,2])
results[,3] <- testing$visitsTumblr
sum((results[3]>results[1])&(results[3]<results[2]))/nrow(results)
set.seed(3523)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testin
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
SVMfit <- svm(CompressiveStrength~.,data=training)
SVMPred <- predict(SVMfit,testing)
mean((SVMPred-testing$CompressiveStrength)^2)^.5
install.packages("crypto")
library(crypto)
will_i_get_rich <- getCoins("xrp")
View(ets1)
installed.packages()
?rpart
library(caret); library(ramdomForest);library(rpart)
library(randomForest)
train <- downloadcsv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", c("", "NA", "#DIV/0!"))
train <- download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", c("", "NA", "#DIV/0!"))
train <- download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", temp, method = "curl")
train <- download.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", "C:\", method = "curl")
getdir()
URL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(URL, destfile = "./data/data.csv", method="curl")
library(curl)
download.file(URL, destfile = "./data/data.csv", method="curl")
download.file(URL, destfile = "data.csv", method="curl")
URL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
x = read.csv(file=url)
x = read.csv(file=URL)
train <- x
remove(x)
URL2<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test = read.csv(file=URL2)
URL2<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test = read.csv(file=URL2, na.strings=c("NA","#DIV/0!", ""))
URL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train = read.csv(file=URL, na.strings=c("NA","#DIV/0!", ""))
head(train)
trai2<-train[,colSums(is.na(train)) == 0]
remove(trai2)
train2<-train[,colSums(is.na(train)) == 0]
test2 <-test[,colSums(is.na(test)) == 0]
head(test2)
samples <- createDataPartition(y=train2$classe, p=0.8, list=FALSE)
library(caret); library(randomForest);library(rpart); library(curl)
samples <- createDataPartition(y=train2$classe, p=0.8, list=FALSE)
sTrain <- train2[samples, ]
sTest <- train2[-samples, ]
dim(sTrain)
dim(sTest)
head(sTrain)
head(sTest)
dim(sTrain)
dim(sTest)
plot(sTrain$classe, col="blue", main="Bar Plot of levels of the variable classe within the subTraining data set", xlab="classe levels", ylab="Frequency")
plot(sTrain$classe,  main="Bar Plot of levels of the variable classe within the subTraining data set", xlab="classe levels", ylab="Frequency")
plot(sTrain$classe,  main="levels of classe within the subTraining", xlab="classe levels", ylab="Frequency")
modelTree <- rpart(classe ~ ., data=sTrain, method="class")
library(rattle)
fancyRpartPlot(modelTree)
?fancyRpartPlot
fancyRpartPlot(modelTree)
text(modelTree,use.n = TRUE,all=TRUE,cex=.8)
fancyRpartPlot(modelTree,cex=1)
fancyRpartPlot(modelTree,cex=0.5)
fancyRpartPlot(modelTree,cex=0.8)
fancyRpartPlot(modelTree,cex=0.7)
fancyRpartPlot(modelTree,cex=0.6)
fancyRpartPlot(modelTree,cex=0.5)
plot(modelTree,uniform = TRUE,
main="Classification Tree")
text(modelTree,use.n = TRUE,all=TRUE,cex=.8)
confusionMatrix(predicTree, sTest$classe)
?confusionMatrix
?confusionMatrix
predicTree <- predict(modelTree, sTest, type = "class")
confusionMatrix(predicTree, sTest$classe)
knitr::opts_chunk$set(echo = TRUE)
samples <- createDataPartition(y=train2$classe, p=0.8, list=FALSE)
library(caret); library(randomForest);library(rpart); library(curl)
samples <- createDataPartition(y=train2$classe, p=0.8, list=FALSE)
sTrain <- train2[samples, ]
sTest <- train2[-samples, ]
dim(sTrain)
dim(sTest)
head(sTrain)
head(sTest)
modelTree <- rpart(classe ~ ., data=sTrain, method="class")
#### Predicting:
predicTree <- predict(modelTree, sTest, type = "class")
#### Plot of the Decision Tree
library(rattle)
fancyRpartPlot(modelTree,cex=0.5)
confusionMatrix(predicTree, sTest$classe)
confusionMatrix(predicTree, sTest$classe)
library(caret); library(randomForest);library(rpart); library(curl)
dim(sTrain)
dim(sTest)
modelForest <- randomForest(classe ~. , data=sTrain, method="class")
predicForest <- predict(modelForest, sTest, type = "class")
confusionMatrix(predicForest, sTest$classe)
samples <- createDataPartition(y=train2$classe, p=0.7, list=FALSE)
sTrain <- train2[samples, ]
sTest <- train2[-samples, ]
dim(sTrain)
dim(sTest)
head(sTrain)
head(sTest)
modelTree <- rpart(classe ~ ., data=sTrain, method="class")
#### Predicting:
predicTree <- predict(modelTree, sTest, type = "class")
#### Plot of the Decision Tree
library(rattle)
fancyRpartPlot(modelTree,cex=0.5)
confusionMatrix(predicTree, sTest$classe)
modelForest <- randomForest(classe ~. , data=sTrain, method="class")
#### Predicting:
predicForest <- predict(modelForest, sTest, type = "class")
#### Now i test the results on our subTesting data set for this first model:
confusionMatrix(predicForest, sTest$classe)
predictResult <- predict(modelForest, test2, type="class")
test = read.csv(file=URL2, na.strings=c("NA","#DIV/0!", ""))
test2 <-test[,colSums(is.na(test)) == 0]
head(test2)
predictResult <- predict(modelForest, test2, type="class")
predictResult <- predict(modelForest, sTest, type="class")
predictResult
dim(train2)
dim(test2)
dim(sTest)
predictResult <- predict(modelForest, test2, type="class")
predictFinal <- predict(modelForest, test2, type="class")
URL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train = read.csv(file=URL, na.strings=c("NA","#DIV/0!", ""))
train2<-train[,colSums(is.na(train)) == 0]
dim(train2)
URL2<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test = read.csv(file=URL2, na.strings=c("NA","#DIV/0!", ""))
test2 <-test[,colSums(is.na(test)) == 0]
dim(test2)
samples <- createDataPartition(y=train2$classe, p=0.7, list=FALSE)
sTrain <- train2[samples, ]
sTest <- train2[-samples, ]
modelForest <- randomForest(classe ~. , data=sTrain, method="class")
#### Predicting:
predicForest <- predict(modelForest, sTest, type = "class")
confusionMatrix(predicForest, sTest$classe)
predictFinal <- predict(modelForest, test2, type="class")
test3 <- rbind(train2[1, ] , test3)
test2 <- rbind(train2[1, ] , test2)
head(train2)
head(test2)
head(train2)
knitr::opts_chunk$set(echo = TRUE)
train2 <-train2[,-c(1:7)]
dim(train2)
test2 <-test2[,-c(1:7)]
dim(test2)
modelForest <- randomForest(classe ~. , data=sTrain, method="class")
library(caret); library(randomForest);library(rpart); library(curl)
modelForest <- randomForest(classe ~. , data=sTrain, method="class")
predicForest <- predict(modelForest, sTest, type = "class")
predictFinal <- predict(modelForest, test2, type="class")
predictFin <- predict(modelForest, test2, type="class")
confusionMatrix(predicForest, sTest$classe)
predictFin <- predict(modelForest, test2, type="class")
View(train2)
View(train2)
View(train2)
View(test2)
View(test2)
View(test)
View(test)
train2<-train[,colSums(is.na(train)) == 0]
test2 <-test[,colSums(is.na(test)) == 0]
trainCol <- names(train2)
testCol <- names(test2)
setdiff(trainCol, testCol)
setdiff(testCol, trainCol)
test2$problem_id
predictFin <-predict(modelForest, newdata = test2[,-which(names(test) %in% "problem_id")])
predictFin <-predict(modelForest, newdata = test2[,-which(names(test) %in% "problem_id")], type="class")
predictFin <-predict(modelForest, type="class", newdata = test2[,-which(names(test) %in% "problem_id")])
train = read.csv(file=URL, na.strings=c("NA","#DIV/0!", ""))
train2<-train[,colSums(is.na(train)) == 0] #remove columns with all values in NA
dim(train2)
classe <- train2$classe #save classe information
train2 <- train2[,sapply(training,is.numeric)] # remove non-numeric variables
train2$classe <- classe; rm(classe) #add classe information and delete variable
dim(train2)
test2 <- test2[,sapply(test,is.numeric)] # remove non-numeric variables
test2 <- test2[,sapply(test2,is.numeric)] # remove non-numeric variables
dim(test2)
dim(train2)
set.seed(7472)
set.seed(7472)
samples <- createDataPartition(y=train2$classe, p=0.7, list=FALSE)
sTrain <- train2[samples, ]
sTest <- train2[-samples, ]
plot(sTrain$classe,  main="levels of classe within the subTraining", xlab="classe levels", ylab="Frequency")
set.seed(7472)
modelForest <- randomForest(classe ~. , data=sTrain, method="class")
predicForest <- predict(modelForest, sTest, type = "class")
VarImportance <- varImp(modelForest)
plot(VarImportance, main = "Most relevant variables", top = 10)
qplot(VarImportance, main = "Most relevant variables", top = 10)
predicForest
train <- subset(train, select=-c(1:6)) #remove row numbers, time stamp and user names
train2<-train[,colSums(is.na(train)) == 0] #remove columns with at least an NA value
classe <- train2$classe #save classe information
train2 <- train2[,sapply(training,is.numeric)] # remove non-numeric variables
train2$classe <- classe; rm(classe) #add classe information and delete variable
dim(train2)
test <- subset(test, select=-c(1:6)) #remove row numbers, time stamp and user names
test2 <-test[,colSums(is.na(test)) == 0] #remove columns with at least a NA value
test2 <- test2[,sapply(test2,is.numeric)] # remove non-numeric variables
dim(test2)
trainCol <- names(train2)
testCol <- names(test2)
setdiff(trainCol, testCol)
setdiff(testCol, trainCol)
test2$problem_id
set.seed(7472)
samples <- createDataPartition(y=train2$classe, p=0.7, list=FALSE)
sTrain <- train2[samples, ]
sTest <- train2[-samples, ]
plot(sTrain$classe,  main="levels of classe within the subTraining", xlab="classe levels", ylab="Frequency")
set.seed(7472)
modelForest <- randomForest(classe ~. , data=sTrain, method="class")
predicForest <- predict(modelForest, sTest, type = "class")
VarImportance <- varImp(modelForest)
VarImportance
View(VarImportance)
View(VarImportance)
plot(VarImportance, main = "Most relevant variables", top = 10)
View(VarImportance)
plot(VarImportance$Overall, main = "Most relevant variables", top = 10)
plot(VarImportance$Overall, main = "Most relevant variables", n= 10)
plot(VarImportance$Overall, main = "Most relevant variables", top= 10)
plot(VarImportance$Overall, main = "Most relevant variables",
top= 10)
plot(VarImportance$Overall, main = "Most relevant variables", top= 10)
View(VarImportance)
VarImport <- varImp(modelForest)
plot(VarImport, main = "Most relevant variables", top= 10)
predictF <-predict(modelForest, type="class", newdata = test2[,-which(names(test) %in% "problem_id")])
t(data.frame(problem_id = test2$problem_id, prediction = predictFin))
t(data.frame(problem_id = test2$problem_id, prediction = predictF))
?plot
plot(VarImport, main = "Most relevant variables")
plot(VarImport$Overall, main = "Most relevant variables")
plot(VarImport,VarImport$Overall, main = "Most relevant variables")
plot(VarImport$,VarImport$Overall, main = "Most relevant variables")
plot(VarImport$Overall, main = "Most relevant variables",top=10)
ImpMeasure<-data.frame(varImp(modelForest)$importance)
ImpMeasure$Vars<-row.names(ImpMeasure)
ImpMeasure[order(-ImpMeasure$Overall),][1:10,]
ImpMeasure<-data.frame(varImp(modelForest)$Overall)
ImpMeasure$Vars<-row.names(ImpMeasure)
ImpMeasure[order(-ImpMeasure$Overall),][1:10,]
ImpMeasure<-data.frame(varImp(modelForest)$Overall)[1:10,]
plot(ImpMeasure)
ImpMeasure$Vars<-row.names(ImpMeasure)
plot(ImpMeasure)
ImpMeasure<-data.frame(varImp(modelForest)$Overall)[1:10,]
ImpMeasure$Vars<-row.names(varImp(modelForest))
plot(ImpMeasure)
ImpMeasure$Vars<-row.names(varImp(modelForest))[1:10,]
ImpMeasure<-data.frame(varImp(modelForest)$Overall)[1:10,]
ImpMeasure$Vars<-row.names(varImp(modelForest))[1:10,]
ImpMeasure<-data.frame(varImp(modelForest)$Overall)[1:10,]
ImpMeasure$Vars<-row.names(varImp(modelForest)[1:10,])
plot(ImpMeasure)
ImpMeasure<-data.frame(varImp(modelForest)$importance)[1:10,]
ImpMeasure$Vars<-row.names(varImp(modelForest)[1:10,])
plot(ImpMeasure)
ImpMeasure<-data.frame(varImp(modelForest)$importance)[1:10,]
plot(ImpMeasure)
plot(VarImport$variable, main = "Most relevant variables",top=10)
VarImport <- varImp(modelForest)
predicForest <- predict(modelForest, sTest, type = "class",importance = TRUE)
VarImport <- varImp(modelForest)
plot(VarImport, main = "Most relevant variables",top=10)
ImpMeasure<-data.frame(varImp(modelForest)$importance)
ImpMeasure$Vars<-row.names(ImpMeasure)
ImpMeasure[order(-ImpMeasure$Overall),][1:3,]
ImpMeasure<-data.frame(varImp(modelForest)$importance)
VarImport <- varImp(modelForest)
plot(VarImport, main = "Most relevant variables")[1:10,]
VarImport <- varImp(modelForest)[1:10,]
plot(VarImport, main = "Most relevant variables")
VarImport <- varImp(modelForest)
VarImport[order(-VarImport$Overall),][1:10,]
VarImport[order(-VarImport$Overall),]
VarImport[order(-VarImport$Overall),][1:3,]
VarImport[order(-VarImport$Overall),][,1:3]
VarImport[order(-VarImport$Overall),][1:10,]
varImpPlot(modelForest)
varImpPlot(modelForest)[1:10,]
varImpPlot(modelForest)[1:5,]
?varImpPlot
varImpPlot(modelForest,n.var = 10)
library(caret); library(randomForest);library(rpart); library(curl); library(lattice); library(ggplot2)
View(train)
URL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train0 = read.csv(file=URL, na.strings=c("NA","#DIV/0!", "")) # detect blanks in variables and change for NA
View(train0)
URL2<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test0 = read.csv(file=URL2, na.strings=c("NA","#DIV/0!", ""))# detect blanks in variables and change for NA
View(test0)
View(train0)
URL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train0 = read.csv(file=URL, na.strings=c("NA","#DIV/0!", ""))#detect blanks in variables and change for NA
train <- subset(train, select=-c(1:6)) #remove row numbers, time stamp and user names
URL<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train = read.csv(file=URL, na.strings=c("NA","#DIV/0!", ""))#detect blanks in variables and change for NA
train <- subset(train, select=-c(1:6)) #remove row numbers, time stamp and user names
train2<-train[,colSums(is.na(train)) == 0] #remove columns with at least an NA value
classe <- train2$classe #save classe information
train2 <- train2[,sapply(training,is.numeric)] # remove non-numeric variables
train2 <- train2[,sapply(train2,is.numeric)] # remove non-numeric variables
train2$classe <- classe; rm(classe) #add classe information and delete variable
install.packages("leaflet")
library(leaflet)
my_map <- leaflet() %>%
addTiles()
my_map
library(leaflet)
my_map <- my_map %>%
addMarkers(lat=39.2980803, lng=-76.5898801,
popup="Jeff Leek's Office")
my_map
my_map <- my_map %>%
addMarkers(lat=10.515415, lng=-66.902591,
popup="La Casa de mi compadre")
my_map
set.seed(2016-04-25)
df <- data.frame(lat = runif(20, min = 39.2, max = 39.3),
lng = runif(20, min = -76.6, max = -76.5))
df %>%
leaflet() %>%
addTiles() %>%
addMarkers()
hopkinsIcon <- makeIcon(
iconUrl = "http://brand.jhu.edu/content/uploads/2014/06/university.shield.small_.blue_.png",
iconWidth = 31*215/230, iconHeight = 31,
iconAnchorX = 31*215/230/2, iconAnchorY = 16
)
hopkinsLatLong <- data.frame(
lat = c(39.2973166, 39.3288851, 39.2906617),
lng = c(-76.5929798, -76.6206598, -76.5469683))
hopkinsLatLong %>%
leaflet() %>%
addTiles() %>%
addMarkers(icon = hopkinsIcon)
opkinsSites <- c(
"<a href='http://www.jhsph.edu/'>East Baltimore Campus</a>",
"<a href='https://apply.jhu.edu/visit/homewood/'>Homewood Campus</a>",
"<a href='http://www.hopkinsmedicine.org/johns_hopkins_bayview/'>Bayview Medical Center</a>",
"<a href='http://www.peabody.jhu.edu/'>Peabody Institute</a>",
"<a href='http://carey.jhu.edu/'>Carey Business School</a>"
)
hopkinsLatLong %>%
leaflet() %>%
addTiles() %>%
addMarkers(icon = hopkinsIcon, popup = hopkinsSites)
hopkinsSites <- c(
"<a href='http://www.jhsph.edu/'>East Baltimore Campus</a>",
"<a href='https://apply.jhu.edu/visit/homewood/'>Homewood Campus</a>",
"<a href='http://www.hopkinsmedicine.org/johns_hopkins_bayview/'>Bayview Medical Center</a>",
"<a href='http://www.peabody.jhu.edu/'>Peabody Institute</a>",
"<a href='http://carey.jhu.edu/'>Carey Business School</a>"
)
hopkinsLatLong %>%
leaflet() %>%
addTiles() %>%
addMarkers(icon = hopkinsIcon, popup = hopkinsSites)
df <- data.frame(lat = runif(500, min = 39.25, max = 39.35),
lng = runif(500, min = -76.65, max = -76.55))
df %>%
leaflet() %>%
addTiles() %>%
addMarkers(clusterOptions = markerClusterOptions())
df <- data.frame(lat = runif(20, min = 39.25, max = 39.35),
lng = runif(20, min = -76.65, max = -76.55))
df %>%
leaflet() %>%
addTiles() %>%
addCircleMarkers()
md_cities <- data.frame(name = c("Baltimore", "Frederick", "Rockville", "Gaithersburg",
"Bowie", "Hagerstown", "Annapolis", "College Park", "Salisbury", "Laurel"),
pop = c(619493, 66169, 62334, 61045, 55232,
39890, 38880, 30587, 30484, 25346),
lat = c(39.2920592, 39.4143921, 39.0840, 39.1434, 39.0068, 39.6418, 38.9784, 38.9897, 38.3607, 39.0993),
lng = c(-76.6077852, -77.4204875, -77.1528, -77.2014, -76.7791, -77.7200, -76.4922, -76.9378, -75.5994, -76.8483))
md_cities %>%
leaflet() %>%
addTiles() %>%
addCircles(weight = 1, radius = sqrt(md_cities$pop) * 30)
leaflet() %>%
addTiles() %>%
addRectangles(lat1 = 37.3858, lng1 = -122.0595,
lat2 = 37.3890, lng2 = -122.0625)
df <- data.frame(lat = runif(20, min = 39.25, max = 39.35),
lng = runif(20, min = -76.65, max = -76.55),
col = sample(c("red", "blue", "green"), 20, replace = TRUE),
stringsAsFactors = FALSE)
df %>%
leaflet() %>%
addTiles() %>%
addCircleMarkers(color = df$col) %>%
addLegend(labels = LETTERS[1:3], colors = c("blue", "red", "green"))
set.seed(2016-04-25)
df <- data.frame(lat = runif(20, min = 39.2, max = 39.3),
lng = runif(20, min = -76.6, max = -76.5))
df %>%
leaflet() %>%
addTiles() %>%
addMarkers()
df <- data.frame(lat = runif(500, min = 39.25, max = 39.35),
lng = runif(500, min = -76.65, max = -76.55))
df %>%
leaflet() %>%
addTiles() %>%
addMarkers(clusterOptions = markerClusterOptions())
df <- data.frame(lat = runif(20, min = 39.25, max = 39.35),
lng = runif(20, min = -76.65, max = -76.55))
df %>%
leaflet() %>%
addTiles() %>%
addCircleMarkers()
md_cities <- data.frame(name = c("Baltimore", "Frederick", "Rockville", "Gaithersburg",
"Bowie", "Hagerstown", "Annapolis", "College Park", "Salisbury", "Laurel"),
pop = c(619493, 66169, 62334, 61045, 55232,
39890, 38880, 30587, 30484, 25346),
lat = c(39.2920592, 39.4143921, 39.0840, 39.1434, 39.0068, 39.6418, 38.9784, 38.9897, 38.3607, 39.0993),
lng = c(-76.6077852, -77.4204875, -77.1528, -77.2014, -76.7791, -77.7200, -76.4922, -76.9378, -75.5994, -76.8483))
md_cities %>%
leaflet() %>%
addTiles() %>%
addCircles(weight = 1, radius = sqrt(md_cities$pop) * 30)
library(plotly)
#example 1
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter")
data("airmiles")
plot_ly(x = time(airmiles), y = airmiles,mode="markers")
plot_ly(x = ~time(airmiles), y = ~airmiles, type = "scatter")
plot_ly(iris, y = ~Petal.Length, color = ~Species, type = "box")
```{r iris, echo = FALSE,include=False}
```{r iris, echo = FALSE,include=False}
shiny::runApp('~/TwitterSentimentAnalysis')
runApp('~/TwitterSentimentAnalysis')
runApp('~/TwitterSentimentAnalysis')
runApp('~/TwitterSentimentAnalysis')
